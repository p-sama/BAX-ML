{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 452 - Machine Learning - HW8 - Digit Classification using Tensorflow & Amazon Sagemaker\n",
    "\n",
    "The task is to understand the concept of deep learning and how it can be easily deployed as a running model using Amazon Sagemaker. I have created a notebook instance on sagemaker and an S3 bucket for the data to be stored on the cloud. The rest of the code has been leveraged entirely from the Tensorflow sample notebook provided on Sagemaker console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Set-up-the-environment\" data-toc-modified-id=\"Set-up-the-environment-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Set up the environment</a></span></li><li><span><a href=\"#Download-the-MNIST-dataset\" data-toc-modified-id=\"Download-the-MNIST-dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Download the MNIST dataset</a></span></li><li><span><a href=\"#Upload-the-data\" data-toc-modified-id=\"Upload-the-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Upload the data</a></span></li><li><span><a href=\"#Custom-function-for-Tensorflow\" data-toc-modified-id=\"Custom-function-for-Tensorflow-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Custom function for Tensorflow</a></span></li><li><span><a href=\"#Create-a-training-job-using-the-sagemaker.TensorFlow-estimator\" data-toc-modified-id=\"Create-a-training-job-using-the-sagemaker.TensorFlow-estimator-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Create a training job using the sagemaker.TensorFlow estimator</a></span></li><li><span><a href=\"#Deploy-the-trained-model-to-prepare-for-predictions\" data-toc-modified-id=\"Deploy-the-trained-model-to-prepare-for-predictions-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Deploy the trained model to prepare for predictions</a></span></li><li><span><a href=\"#Invoking-the-endpoint\" data-toc-modified-id=\"Invoking-the-endpoint-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Invoking the endpoint</a></span></li><li><span><a href=\"#Deleting-the-endpoint\" data-toc-modified-id=\"Deleting-the-endpoint-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Deleting the endpoint</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "('Writing', 'data/train.tfrecords')\n",
      "('Writing', 'data/validation.tfrecords')\n",
      "('Writing', 'data/test.tfrecords')\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "import tensorflow as tf\n",
    "\n",
    "data_sets = mnist.read_data_sets('data', dtype=tf.uint8, reshape=False, validation_size=5000)\n",
    "\n",
    "utils.convert_to(data_sets.train, 'train', 'data')\n",
    "utils.convert_to(data_sets.validation, 'validation', 'data')\n",
    "utils.convert_to(data_sets.test, 'test', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-ml-ps/data/mnist'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', bucket='sagemaker-ml-ps', key_prefix='data/mnist')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom function for Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\r\n",
      "import tensorflow as tf\r\n",
      "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\r\n",
      "\r\n",
      "INPUT_TENSOR_NAME = 'inputs'\r\n",
      "SIGNATURE_NAME = 'predictions'\r\n",
      "\r\n",
      "LEARNING_RATE = 0.001\r\n",
      "\r\n",
      "\r\n",
      "def model_fn(features, labels, mode, params):\r\n",
      "    # Input Layer\r\n",
      "    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-1, 28, 28, 1])\r\n",
      "\r\n",
      "    # Convolutional Layer #1\r\n",
      "    conv1 = tf.layers.conv2d(\r\n",
      "        inputs=input_layer,\r\n",
      "        filters=32,\r\n",
      "        kernel_size=[5, 5],\r\n",
      "        padding='same',\r\n",
      "        activation=tf.nn.relu)\r\n",
      "\r\n",
      "    # Pooling Layer #1\r\n",
      "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\r\n",
      "\r\n",
      "    # Convolutional Layer #2 and Pooling Layer #2\r\n",
      "    conv2 = tf.layers.conv2d(\r\n",
      "        inputs=pool1,\r\n",
      "        filters=64,\r\n",
      "        kernel_size=[5, 5],\r\n",
      "        padding='same',\r\n",
      "        activation=tf.nn.relu)\r\n",
      "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\r\n",
      "\r\n",
      "    # Dense Layer\r\n",
      "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\r\n",
      "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\r\n",
      "    dropout = tf.layers.dropout(\r\n",
      "        inputs=dense, rate=0.4, training=(mode == Modes.TRAIN))\r\n",
      "\r\n",
      "    # Logits Layer\r\n",
      "    logits = tf.layers.dense(inputs=dropout, units=10)\r\n",
      "\r\n",
      "    # Define operations\r\n",
      "    if mode in (Modes.PREDICT, Modes.EVAL):\r\n",
      "        predicted_indices = tf.argmax(input=logits, axis=1)\r\n",
      "        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\r\n",
      "\r\n",
      "    if mode in (Modes.TRAIN, Modes.EVAL):\r\n",
      "        global_step = tf.train.get_or_create_global_step()\r\n",
      "        label_indices = tf.cast(labels, tf.int32)\r\n",
      "        loss = tf.losses.softmax_cross_entropy(\r\n",
      "            onehot_labels=tf.one_hot(label_indices, depth=10), logits=logits)\r\n",
      "        tf.summary.scalar('OptimizeLoss', loss)\r\n",
      "\r\n",
      "    if mode == Modes.PREDICT:\r\n",
      "        predictions = {\r\n",
      "            'classes': predicted_indices,\r\n",
      "            'probabilities': probabilities\r\n",
      "        }\r\n",
      "        export_outputs = {\r\n",
      "            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\r\n",
      "        }\r\n",
      "        return tf.estimator.EstimatorSpec(\r\n",
      "            mode, predictions=predictions, export_outputs=export_outputs)\r\n",
      "\r\n",
      "    if mode == Modes.TRAIN:\r\n",
      "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\r\n",
      "        train_op = optimizer.minimize(loss, global_step=global_step)\r\n",
      "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n",
      "\r\n",
      "    if mode == Modes.EVAL:\r\n",
      "        eval_metric_ops = {\r\n",
      "            'accuracy': tf.metrics.accuracy(label_indices, predicted_indices)\r\n",
      "        }\r\n",
      "        return tf.estimator.EstimatorSpec(\r\n",
      "            mode, loss=loss, eval_metric_ops=eval_metric_ops)\r\n",
      "\r\n",
      "\r\n",
      "def serving_input_fn(params):\r\n",
      "    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [None, 784])}\r\n",
      "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\r\n",
      "\r\n",
      "\r\n",
      "def read_and_decode(filename_queue):\r\n",
      "    reader = tf.TFRecordReader()\r\n",
      "    _, serialized_example = reader.read(filename_queue)\r\n",
      "\r\n",
      "    features = tf.parse_single_example(\r\n",
      "        serialized_example,\r\n",
      "        features={\r\n",
      "            'image_raw': tf.FixedLenFeature([], tf.string),\r\n",
      "            'label': tf.FixedLenFeature([], tf.int64),\r\n",
      "        })\r\n",
      "\r\n",
      "    image = tf.decode_raw(features['image_raw'], tf.uint8)\r\n",
      "    image.set_shape([784])\r\n",
      "    image = tf.cast(image, tf.float32) * (1. / 255)\r\n",
      "    label = tf.cast(features['label'], tf.int32)\r\n",
      "\r\n",
      "    return image, label\r\n",
      "\r\n",
      "\r\n",
      "def train_input_fn(training_dir, params):\r\n",
      "    return _input_fn(training_dir, 'train.tfrecords', batch_size=100)\r\n",
      "\r\n",
      "\r\n",
      "def eval_input_fn(training_dir, params):\r\n",
      "    return _input_fn(training_dir, 'test.tfrecords', batch_size=100)\r\n",
      "\r\n",
      "\r\n",
      "def _input_fn(training_dir, training_filename, batch_size=100):\r\n",
      "    test_file = os.path.join(training_dir, training_filename)\r\n",
      "    filename_queue = tf.train.string_input_producer([test_file])\r\n",
      "\r\n",
      "    image, label = read_and_decode(filename_queue)\r\n",
      "    images, labels = tf.train.batch(\r\n",
      "        [image, label], batch_size=batch_size,\r\n",
      "        capacity=1000 + 3 * batch_size)\r\n",
      "\r\n",
      "    return {INPUT_TENSOR_NAME: images}, labels\r\n"
     ]
    }
   ],
   "source": [
    "!cat 'mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training job using the sagemaker.TensorFlow estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-866319475440\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-tensorflow-py2-cpu-2018-03-06-06-36-47-338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................\n",
      "\u001b[31mexecuting startup script (first run)\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:23,443 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:23,443 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:24,982 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:25,884 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:26,006 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:{\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"algo-2:2222\"], \"ps\": [\"algo-1:2223\", \"algo-2:2223\"], \"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"master\"}}\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:---------------------------------------------------------\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:going to training\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:26,055 INFO - root - creating RunConfig:\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:26,055 INFO - root - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:26.055866: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:26,056 INFO - root - creating the estimator\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Using config: {'_model_dir': u's3://sagemaker-us-east-1-866319475440/sagemaker-tensorflow-py2-cpu-2018-03-06-06-36-47-338/checkpoints', '_save_checkpoints_secs': 300, '_num_ps_replicas': 2, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': u'master', '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd0c1a15950>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31m, '_num_worker_replicas': 2, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://algo-1:2222', '_log_step_count_steps': 100}\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:26,056 INFO - root - creating Experiment:\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:26,056 INFO - root - {'min_eval_frequency': 1000}\u001b[0m\n",
      "\u001b[31mE0306 06:42:26.059352289      62 ev_epoll1_linux.c:1051]     grpc epoll fd: 5\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:26.066542: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> algo-1:2222}\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:26.066562: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2223, 1 -> algo-2:2223}\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:26.066576: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> algo-2:2222}\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:26.068447: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:267: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mMonitors are deprecated. Please use tf.train.SessionRunHook.\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:26.150019: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> localhost:2222}\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:26.150043: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> algo-1:2223, 1 -> algo-2:2223}\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:26.150053: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> algo-2:2222}\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:26.150194: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[32mexecuting startup script (first run)\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:28,738 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:28,738 INFO - root - starting train task\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:30,393 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31,316 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31,438 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:{\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"algo-2:2222\"], \"ps\": [\"algo-1:2223\", \"algo-2:2223\"], \"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"worker\"}}\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:---------------------------------------------------------\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:going to training\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31.486067: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31,486 INFO - root - creating RunConfig:\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31,486 INFO - root - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31,486 INFO - root - creating the estimator\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:Using config: {'_model_dir': u's3://sagemaker-us-east-1-866319475440/sagemaker-tensorflow-py2-cpu-2018-03-06-06-36-47-338/checkpoints', '_save_checkpoints_secs': 300, '_num_ps_replicas': 2, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': u'worker', '_environment': u'cloud', '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbd1d395950>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\u001b[0m\n",
      "\u001b[32m}\u001b[0m\n",
      "\u001b[32m, '_num_worker_replicas': 2, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': u'grpc://algo-2:2222', '_log_step_count_steps': 100}\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31,487 INFO - root - creating Experiment:\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31,487 INFO - root - {'min_eval_frequency': 1000}\u001b[0m\n",
      "\u001b[32mE0306 06:42:31.489331771       1 ev_epoll1_linux.c:1051]     grpc epoll fd: 5\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31.497037: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> algo-1:2222}\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31.497066: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> algo-1:2223, 1 -> algo-2:2223}\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31.497072: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222}\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31.497741: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31.497900: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> algo-1:2222}\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31.497924: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> algo-1:2223, 1 -> localhost:2223}\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31.497934: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> algo-2:2222}\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31.498563: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[32m2018-03-06 06:42:31.994844: I tensorflow/core/distributed_runtime/master_session.cc:1004] Start master session b506ed006d7e5cb0 with config: gpu_options { per_process_gpu_memory_fraction: 1 } allow_soft_placement: true\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: global_step, conv2d/kernel, conv2d/bias, conv2d_1/kernel, conv2d_1/bias, dense/kernel, dense/bias, dense_1/kernel, dense_1/bias, beta1_power, beta2_power, conv2d/kernel/Adam, conv2d/kernel/Adam_1, conv2d/bias/Adam, conv2d/bias/Adam_1, conv2d_1/kernel/Adam, conv2d_1/kernel/Adam_1, conv2d_1/bias/Adam, conv2d_1/bias/Adam_1, dense/kernel/Adam, dense/kernel/Adam_1, dense/bias/Adam, dense/bias/Adam_1, dense_1/kernel/Adam, dense_1/kernel/Adam_1, dense_1/bias/Adam, dense_1/bias/Adam_1, ready: None\u001b[0m\n",
      "\u001b[31m2018-03-06 06:42:32.333908: I tensorflow/core/distributed_runtime/master_session.cc:1004] Start master session 1c744dfa71906148 with config: gpu_options { per_process_gpu_memory_fraction: 1 } allow_soft_placement: true\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving checkpoints for 1 into s3://sagemaker-us-east-1-866319475440/sagemaker-tensorflow-py2-cpu-2018-03-06-06-36-47-338/checkpoints/model.ckpt.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mINFO:tensorflow:loss = 2.333385, step = 0\u001b[0m\n",
      "\u001b[32m2018-03-06 06:43:02.112376: I tensorflow/core/distributed_runtime/master_session.cc:1004] Start master session 74296cad934d7c88 with config: gpu_options { per_process_gpu_memory_fraction: 1 } allow_soft_placement: true\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:loss = 0.16479854, step = 82\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 3.71356\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.111868255, step = 117 (29.912 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.82797\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:loss = 0.020259047, step = 274 (28.361 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.97166\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.080218576, step = 329 (30.713 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.90278\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:loss = 0.02698376, step = 465 (27.722 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.63655\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.066274956, step = 534 (30.355 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.67109\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:loss = 0.026596665, step = 660 (28.924 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.91668\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.045887195, step = 744 (30.851 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.72179\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:loss = 0.04160817, step = 853 (28.705 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:global_step/sec: 6.70243\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:loss = 0.0054136934, step = 947 (30.449 sec)\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving checkpoints for 1001 into s3://sagemaker-us-east-1-866319475440/sagemaker-tensorflow-py2-cpu-2018-03-06-06-36-47-338/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:Loss for final step: 0.0025114974.\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:writing success training\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Loss for final step: 0.0038302538.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Starting evaluation at 2018-03-06-06:45:26\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Restoring parameters from s3://sagemaker-us-east-1-866319475440/sagemaker-tensorflow-py2-cpu-2018-03-06-06-36-47-338/checkpoints/model.ckpt-1001\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [1/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [2/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [3/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [4/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [5/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [6/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [7/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [8/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [9/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [10/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [11/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [12/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [13/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [14/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [15/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [16/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [17/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [18/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [19/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [20/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [21/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [22/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [23/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [24/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [25/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [26/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [27/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [28/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [29/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [30/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [31/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [32/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [33/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [34/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [35/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [36/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [37/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [38/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [39/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [40/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [41/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [42/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [43/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [44/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [45/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [46/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [47/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [48/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [49/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [50/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [51/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [52/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [53/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [54/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [55/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [56/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [57/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [58/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [59/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [60/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [61/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [62/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [63/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [64/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [65/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [66/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [67/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [68/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [69/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [70/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [71/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [72/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [73/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [74/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [75/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [76/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [77/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [78/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [79/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [80/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [81/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [82/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [83/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [84/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [85/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [86/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [87/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [88/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [89/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [90/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [91/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [92/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [93/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [94/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [95/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [96/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [97/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [98/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [99/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Evaluation [100/100]\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Finished evaluation at 2018-03-06-06:45:33\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Saving dict for global step 1001: accuracy = 0.9854, global_step = 1001, loss = 0.04483322\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Restoring parameters from s3://sagemaker-us-east-1-866319475440/sagemaker-tensorflow-py2-cpu-2018-03-06-06-36-47-338/checkpoints/model.ckpt-1001\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:Assets added to graph.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:No assets to write.\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:SavedModel written to: s3://sagemaker-us-east-1-866319475440/sagemaker-tensorflow-py2-cpu-2018-03-06-06-36-47-338/checkpoints/export/Servo/temp-1520318734/saved_model.pb\u001b[0m\n",
      "\u001b[31mINFO:tensorflow:writing success training\u001b[0m\n",
      "\u001b[31m2018-03-06 06:45:40,125 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-03-06 06:45:40,281 INFO - tf_container.serve - Downloaded saved model at /opt/ml/model/export/Servo/1520318734/saved_model.pb\u001b[0m\n",
      "\u001b[31m2018-03-06 06:45:40,293 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (2): s3.amazonaws.com\u001b[0m\n",
      "\u001b[32mINFO:tensorflow:master algo-1 is down, stopping parameter server\u001b[0m\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "mnist_estimator = TensorFlow(entry_point='mnist.py',\n",
    "                             role=role,\n",
    "                             training_steps=1000, \n",
    "                             evaluation_steps=100,\n",
    "                             train_instance_count=2,\n",
    "                             train_instance_type='ml.c4.xlarge')\n",
    "\n",
    "mnist_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Deploy the trained model to prepare for predictions\n",
    "\n",
    "The deploy() method creates an endpoint which serves prediction requests in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-tensorflow-py2-cpu-2018-03-06-06-36-47-338\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-tensorflow-py2-cpu-2018-03-06-06-36-47-338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "mnist_predictor = mnist_estimator.deploy(initial_instance_count=1,\n",
    "                                             instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "========================================\n",
      "label is 7\n",
      "prediction is 7\n",
      "========================================\n",
      "label is 2\n",
      "prediction is 2\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 0\n",
      "prediction is 0\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 1\n",
      "prediction is 1\n",
      "========================================\n",
      "label is 4\n",
      "prediction is 4\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n",
      "========================================\n",
      "label is 5\n",
      "prediction is 5\n",
      "========================================\n",
      "label is 9\n",
      "prediction is 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "for i in range(10):\n",
    "    data = mnist.test.images[i].tolist()\n",
    "    tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[1, len(data)], dtype=tf.float32)\n",
    "    predict_response = mnist_predictor.predict(tensor_proto)\n",
    "    \n",
    "    print(\"========================================\")\n",
    "    label = np.argmax(mnist.test.labels[i])\n",
    "    print(\"label is {}\".format(label))\n",
    "    prediction = predict_response['outputs']['classes']['int64Val'][0]\n",
    "    print(\"prediction is {}\".format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-tensorflow-py2-cpu-2018-03-06-06-36-47-338\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(mnist_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
